# TP-CI-CD

## Information g√©n√©rales

Groupe:
- Adrien Raimbault
- Dziyana Tsetserava
- Muriel Paraire


Langage de programmation choisie:
- Python


## Questions

### 1. Cr√©ez un fichier docker-compose.yml et ajoutez-y un service db s'appuyant sur l'imageDocker postrges:latest.

Nous avons donc cr√©√© un service de nom `db` de mani√®re suivante :
```yml
services:
  db:
    # utiliser la derni√®re image postgres
    image: postgres:latest
    # redemarrer en cas d'√©chec ou de crash
    restart: always
    # prise en compte des valeurs pr√©cis√© par l'utilisateur
    environment:
      - POSTGRES_PASSWORD=${CITY_API_DB_PWD:?error}
      - POSTGRES_USER=${CITY_API_DB_USER?:error}
      - POSTGRES_DB=city_api
    # le port sur lequel on peut joindre la base de donn√©es
    expose:
      - ${CITY_API_DB_PORT:?error}
    volumes:
      # l'endroit de sauvegarde
      - /var/lib/postgres
      # le script pour cr√©er la table city
      - ./db/init.sql:/docker-entrypoint-initdb.d/create_city.sql
      # le script pour ins√©rer les donn√©es
      - ./db/populate.sql:/docker-entrypoint-initdb.d/populate_city.sql
```

Par la suite, nous avons aussi complet√© notre fichier *docker-compose.yml* pour y ajouter un containeur avec notre API : 

```yml
  python:
    # le dockerfile pour lancer l'application
    build:
      context: .
      dockerfile: python.dockerfile
    # le fichier d'environnement
    env_file:
      - .env
    # l'api a besoin que la base de donn√©es sois cr√©er pour pouvoir l'utiliser
    depends_on:
      - db
    # le port de l'application d√©fini par l'utilisateur
    ports:
      - ${CITY_API_PORT:?error}:${CITY_API_PORT:?error}
```

### 2. Cr√©ez une base de donn√©es  city_api  avec une table  city  contenant les colonnes suivantes :
- **id** , un entier non sign√© non nul, cl√© primaire de la colonne ;
- ***department_code** , une cha√Æne de caract√®res non nulle ;
- **insee_code** , une cha√Æne de caract√®res ;
- **zip_code** , une cha√Æne de caract√®res ;
- **name** , une cha√Æne de caract√®res non nulle ;
- **lat** , un flottant non nul ;
- **lon** , un flottant non nul.

Pour cr√©er cette table, nous avons opt√© de cr√©er un script [init.sql](db/init.sql) que nous passerons √† docker lors de l'initialisation du conteneur. 

Il s'√©x√©cutera et cr√©era la table dans la base de donn√©es qui sera popul√©e par le script [populate.sql](db/populate.sql).

### 3. Dans le langage de votre choix, cr√©ez un service web ayant les sp√©cifications suivantes :
  - `POST /city` avec pour corps de la requ√™te un JSON au format d√©crit plus bas doit retourner un code `201` et enregistrer la ville dans la base de donn√©es ;
  - `GET /city` doit retourner un code `200` avec la liste des villes au format JSON ;
  - `GET /_health` doit retourner un code `204`.

Pour cette API, nous avons choisi d'utiliser [Flask](https://flask.palletsprojects.com/en/2.3.x/), un framework web Python qui permet de d√©marrer rapidement une application web; le but de ce projet n'√©tant pas de passer du temps sur l'application mais sur la partie CI/CD.

Le code de l'application se trouve dans le fichier [application.py](app/application.py).

Voici un diagramme de s√©quence pour r√©sumer les routes :

```mermaid
sequenceDiagram
    actor C as Client
    participant A as API
    participant D as Database
    C->>+A: GET /city
    A->>-C: HTTP 200 OK
    C->>+A: POST /city
    A-->>D: INSERT INTO city
    D-->>A: OK
    A->>-C: HTTP 201 Created
    C->>+A: GET /_health
    A->>-C: HTTP 204 No Content
```


### 4. √âcrivez les tests suivants :
  - un test qui s'assure que l'insertion dans la base de donn√©es fonctionne correctement ;
  - un test qui s'assure que la r√©cup√©ration de la liste des villes fonctionne correctement ;
  - un test qui s'assure que l'endpoint de healthcheck fonctionne correctement.

Concernant les tests, nous avons d√©cid√© d'utiliser [Pytest](https://docs.pytest.org/en/7.4.x/).

Pytest nous permet d'utiliser la commande : 

```bash
python3 -m pytest -c app/tests/pytest.ini
```

Cette commande va trouver toutes les fonctions commen√ßant par *test_* et va essayer de les ex√©cuter.

PS : le fichier [pytest.ini](app/tests/pytest.ini) donne √† pytest la configuration n√©cessaire. Comme nous utilisons une autre base de donn√©es pour les tests (cf. question 7), le fichier *pytest.init* permet de modifier les variables d'environnement et se connecter ainsi vers la base de donn√©es de tests sur un autre port que celui de la base de donn√©es de production.


### 5. √âcrivez un fichier Dockerfile √† la racine de votre projet. Testez que votre image Docker est correcte.

Voici le fichier [python.dockerfile](./python.dockerfile) qui r√©cup√®re l'image python, installe les d√©pendances d'apr√®s le fichier [requirement.txt](./requirements.txt), copie les fichiers pr√©sents dans le dossier *./app* et run l'application.

```yml
FROM python:3.10-alpine

COPY ./requirements.txt /app/requirements.txt

# set workdir to app
WORKDIR /app

# install dependencies
RUN pip install -r requirements.txt

# Copy app folder
COPY ./app /app

# configure the container to run in an executed manner
ENTRYPOINT [ "python" ]

CMD ["application.py" ]
```

PS : Nous utilisons ici une image **python:3.10-alpine** pour diminuer la taille de l'image, les images bas√©s sur alpine √©tants g√©n√©rallement plus l√©g√®res.

### 6. √âcrivez un workflow GitHub Actions ci pour qu'un linter soit ex√©cut√© √† chaque push.

Pour √©crire notre premier workflow, nous nous sommes renseign√©s sur le fonctionnement de Github concernant l'int√©gration continue. Apr√®s quelques recherches, nous avons mis en place un runner h√©berg√© sur l'un de nos serveurs √† Polytech. C'est tr√®s simple √† mettre √† d√©marrer et cela √©vite d'avoir d'√™tre limit√© en temps pendant le fonctionnement de la CI.

Ensuite nous avons ajout√© une action permettant de r√©cup√©rer l'√©tat actuel du projet.

Enfin avant de lancer le linter, nous installons les d√©pendances pour √©viter que Pylint remonte des erreurs li√©s √† cela.

Le fichier [lint_and_test_ci.yml](.github/workflows/lint_and_test_ci.yml) (anciennement *github_ci.yml*) permet de r√©aliser toutes ces √©tapes et d'√©xecuter [pylint](https://pypi.org/project/pylint/) qui failera si le score est inf√©rieur √† 5/10.

```yml
name: GitHub Actions CI
run-name: ${{ github.actor }} is testing out GitHub Actions üöÄ
on: [push]

jobs:
  python-ci:
    runs-on: self-hosted
    steps:
          - uses: actions/checkout@v3

          - name: Install dependencies
            run: |
              python3 -m pip install --upgrade pip
              pip3 install pytest
              pip3 install pylint
              if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

          - name: Analysing the code with pylint
                  run: |
                    pylint --fail-under=5 app/*
```

### 7. Modifiez le workflow pour que les tests s'ex√©cutent √† chaque push.

#### Solution N¬∞1

Ici les tests ont d√©j√† √©t√© √©crits. Il ne reste plus qu'√† les inclure dans le workflow.

La premi√®re id√©e que nous avons eu √©tait de cr√©er un duplicat de notre base de donn√©es postgres (nomm√©e "postgres-test") qui serait lanc√© avec la commande

```bash
docker compose up postgres-test
```

Lors des tests nous avons eu des probl√®mes pour connecter la base de donn√©es et l'application. L'application se lan√ßait apr√®s la base de donn√©es mais pas assez tard pour que la base de donn√©es puisse accepter la connexion.

#### Solution N¬∞2

Apr√®s renseignement, nous avons d√©couvert les [services](https://docs.github.com/en/actions/using-containerized-services/about-service-containers). Ils permettent de lancer des conteneurs docker et sont g√©r√©s par le workflow. Ils sont tr√®s configurables et permettent de pr√©parer le "terrain" avant de lancer le workflow.

Apr√®s impl√©mentation de cette id√©e, voici notre workflow (fichier [lint_and_test_ci.yml](.github/workflows/lint_and_test_ci.yml)) : 

```yml
name: GitHub Actions CI
run-name: ${{ github.actor }} is testing out GitHub Actions üöÄ
on: [push]

jobs:
  python-ci:
    runs-on: self-hosted
    # Service containers to run with `container-job`
    services:
      postgres-test:
        image: postgres
        # Provide the env for postgres
        env:
          POSTGRES_PASSWORD: test
          POSTGRES_USER: test
          POSTGRES_HOST: localhost
          POSTGRES_PORT: 5436
          POSTGRES_DB: city_api
        # Set health checks to wait until postgres has started
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          # Maps tcp port 5432 on service container to port 5436 on the host
          - 5436:5432

    steps:
      - uses: actions/checkout@v3

      - name: Install dependencies
        run: |
          python3 -m pip install --upgrade pip
          pip3 install pytest
          pip3 install pylint
          if [ -f requirements.txt ]; then pip install -r requirements.txt; fi

      - name: Analysing the code with pylint
        run: |
          pylint --fail-under=5 app/*
      
      - name: Testing code
        run: |
          psql -U test city_api -h localhost -p 5436 -f db/init.sql
          python3 -m pytest -c app/tests/pytest.ini
        env:
          PGPASSWORD: test

      - run: echo "üçè This job's status is ${{ job.status }}."
```

### 8. Modifiez le workflow pour qu'un build de l'image Docker soit r√©alis√© √† chaque push.

N'ayant pas d'id√©e pour build l'image, nous avons fait des recherches et sommes tomb√©s sur le marketplace de Github.

Le market place offre des actions pr√©fabriqu√©es pour simplifier le travail d'int√©gration continue avec Github. Nous avons d√©cid√©s d'utiliser une action officielle de docker : `docker/build-push-action@v4`.
Cette action permet de build l'image docker et de la push sur le registry desir√©e, ce qui nous permet de r√©aliser la question 8 & 9 en m√™me temps.

Voici le job d√©di√© au build et au push :

```yml
name: GitHub Release CI
run-name: ${{ github.actor }} is testing out GitHub Actions üöÄ

on: [push]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository_owner }}/city-api

jobs:
  build:
    name: Build Docker image
    runs-on: self-hosted
    permissions:
      packages: write
    steps:
      - uses: actions/checkout@v3
        name: Check out the repository

      - uses: docker/login-action@v2
        name: Log in to the container registry
        with:
          registry: ${{ env.REGISTRY }}
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - uses: docker/build-push-action@v4
        name: Build and push the Docker image
        with:
          file: python.dockerfile
          push: true
```

#### Remarques :
  - On sp√©cifie dans les variables d'environnement la registry utilis√©e pour push l'image (ghcr.io ici).
  - Il est n√©cessaire de se connecter en amont via l'action `docker/login-action@v2`. Cette action cr√©e un token d'authentification `GITHUB_TOKEN` (ou utilise celui existant le cas ech√©ant) pour se connecter √† la registry de Github.

### 9. Modifiez le workflow pour que l'image Docker soit push sur ghcr.io avec pour tag city-api:latest.

Nous avons pr√©c√©demment build & push l'image docker. Pour rajouter un tag nous pouvons simplement le pr√©ciser avec l'argument `tags` :

```yml
- uses: docker/build-push-action@v4
        name: Build and push the Docker image
        with:
          file: python.dockerfile
          push: true
          tags: [latest]
```

### 10. √âcrivez un workflow GitHub Actions release qui, lorsqu'un tag au format vX.X.X soit pouss√© build et push l'image Docker avec un tag city-api:X.X.X.

On veut maintenant pouvoir passer un tag *version*. Le but est que si le commit ne contient pas de tag au format *vX.X.X*, l'image n'est ni build ni push.
Pour cela, il suffit juste de modifier le d√©clancheur (trigger) pour le lancer seulement un un tag au format *vX.X.X* est pr√©sent :

```yml
on:
  push:
    tags:
      - 'v*.*.*'
```

Ensuite il faut donner le m√™me tag √† l'image push sur la registry Github. 
On utilise ici une autre action officielle docker : `docker/metadata-action@v4`. Elle permet d'extraire les tags d'un commit.

Il suffit de rajouter un bloc dans notre fichier :

```yml
- uses: docker/metadata-action@v4
        name: Extract metadata (tags, labels) for Docker
        id: meta
        with:
          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
          tags: |
            type=semver,pattern={{version}}
            latest
```

Enfin on modifie la ligne *tags* pour ajouter √† l'image les m√™me tags :


```yml
- uses: docker/build-push-action@v4
      name: Build and push the Docker image
      with:
        file: python.dockerfile
        push: true
        tags: ${{ steps.meta.outputs.tags }}
```

### 11. Installez Minikube sur votre machine local.

Pour installer minikube on peut suivre la documentation √† [https://minikube.sigs.k8s.io/docs/start/](https://minikube.sigs.k8s.io/docs/start/)

```sh
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo install minikube-linux-amd64 /usr/local/bin/minikube
```
Pour lancer minikube il suffit d'utiliser la commande 
```sh
minikube start
```

### 12. √âcrivez un chart Helm de d√©ploiement de l'application.

#### Installer helm :

Documentation officielle : https://helm.sh/docs/intro/install/


Pour utiliser le helm chart de mani√®re g√©n√©rale : 
```
helm install <name> <path_to_helm_directory>
```

Pour l'arr√®ter :
```
helm uninstall <name>
```

Pour initialiser un helm chart, nous en avons utilis√© `helm create city-api` pour cr√©er un g√©n√©rique duquel on pouvais partir.

Pour le moment, l'API est accessible depuis l'ext√©rieur du cluster, mais en changeant la valeur du type de service √† ClusterIP on peut le restreindre.

```yaml
service:
  type: NodePort # change to ClusterIP to block access from outside the Clutser
```
L'utilisateur et le mot de passe sont d√©fini dans le fichier city-api/values.yaml


### 13. D√©ployez votre application dans votre Minikube.


Pour d√©ployer notre application dans le minikube il suffit de se positionner dans ce directoire et de installer le helm chart:
```
helm install city-api ./city-api
```

Pour r√©cuperer le URL du service de l'API :
```
minikube service --all
```

Vous trouverez le service `api` avec une url √† c√¥t√©.

Il se peut que vous devez attendre un peu avant de pouvoir acc√®der √† l'application.

Pour arr√™ter l'application :
```
helm uninstall city-api
```

### 14. Ajouter un endpoint /metrics compatible Prometheus (des libs sont disponibles).

Afin de calculer des metrics pour Prometheus, nous avons utilis√© le client officiel Python `prometheus-client`. En particulier, une classe `Counter` utilis√©e pour calculer le nombre de requ√™tes et la fonction `generate_latest` qui renvoie les m√©triques sous forme de string compatible Prometheus.

Lors de la cr√©ation d'un `Counter` pour notre application, nous avons ajout√© deux labels qui peuvent √™tre utilis√©es ult√©rieurement pour filtrer les donn√©es de m√©trique¬†:

```python
http_requests_total = Counter('http_requests_total', 'Requests to city API', ['method', 'code'])
```

Ainsi, chaque fois qu'une requ√™te est faite √† l'API, le programme ex√©cute une ligne de code comme celle-ci pour l'enregistrer dans Prometheus¬†:

```python
http_requests_total.labels(method='get', code=200).inc()
```

L'exemple ci-dessus correspond √† une requ√™te `GET /city` r√©ussie.

Tout ce que nous avons √† faire dans la requ√™te `GET /metrics` est de renvoyer les m√©triques sous forme de string √† l'aide d'une fonction correspondante¬†:

```python
return generate_latest()
```

### 15. Ajoutez un Prometheus dans votre docker-compose qui scrappe les m√©triques de votre application.

Pour ajouter Prometheus en tant que service dans un fichier *docker-compose.yml*¬†:

```yml
  prometheus:
   image: prom/prometheus:latest
   volumes:
     - ./metrics/prometheus.yaml:/etc/prometheus/prometheus.yml
     - prometheus:/prometheus
   ports:
     - ${PROMETHEUS_PORT:?error}:9090
```

Nous ajoutons √©galement une variable d'environnement `PROMETHEUS_PORT` dans le fichier *.env* et un volume `prometheus` √† la fin pour √©viter de perdre des donn√©es de m√©triques √† chaque red√©marrage de nos conteneurs¬†:

```yml
volumes:
  prometheus:
```

Enfin, nous cr√©ons un fichier *metrics/prometheus.yaml* qui indique √† Prometheus o√π supprimer les m√©triques (√† partir d'un endpoint `python:2022/metrics`)¬†:

```yml
scrape_configs:
  - job_name: city_api
    metrics_path: /metrics
    static_configs:
      - targets:
        - python:2022
        # Same value as in CITY_API_PORT in .env
```

Nous utilisons `python` au lieu de `localhost` puisque tous les services s'ex√©cutent sur le m√™me r√©seau Docker.

### 16. Ajoutez un Grafana dans votre docker-compose et cr√©ez y un dahsboard pour monitorer votre application.

Pour ajouter Grafana √† docker-compose¬†:

```yml
  grafana:
   image: grafana/grafana:latest
   volumes:
     - ./metrics/datasource.yaml:/etc/grafana/provisioning/datasources/datasource.yaml
     - ./metrics/dashboards.yaml:/etc/grafana/provisioning/dashboards/dashboards.yaml
     - ./metrics/dashboard.json:/var/lib/grafana/dashboards/dashboard.json
   ports:
     - ${GRAFANA_PORT:?error}:3000
```
Nous devons √©galement ajouter trois fichiers suppl√©mentaires √† notre dossier *metrics*¬†:

1. [datasource.yaml](metrics/datasource.yaml)

```yml
apiVersion: 1

datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    isDefault: true
    url: http://prometheus:9090/
```
Ce fichier indique √† Grafana o√π trouver les donn√©es de m√©triques. Dans notre cas, il doit acc√©der au service `prometheus` sur le m√™me r√©seau docker sur le port 9090.

2. [dashboards.yaml](metrics/dashboards.yaml)

```yml
apiVersion: 1

providers:
  - name: 'provisionned dashboards'
    orgId: 1
    folder: ''
    folderUid: ''
    type: file
    disableDeletion: false
    editable: true
    updateIntervalSeconds: 10
    allowUiUpdates: false
    options:
      path: /var/lib/grafana/dashboards
```

Ce fichier indique √† Grafana o√π trouver un fichier JSON pour un dashboard que nous allons cr√©er.

3. [dashboard.json](metrics/dashboard.json)

Ce fichier est l'endroit o√π nous pouvons enregistrer toutes les configurations de notre dashboard Grafana. Une fois que nous avons cr√©√© ou mis √† jour un dashboard via l'interface utilisateur Grafana, nous devons aller dans *Dashboard Settings -> JSON Model* et copier un mod√®le JSON dans le fichier *dashboard.json*. De cette fa√ßon, si nous red√©marrons les conteneurs, nous pourrons toujours voir le m√™me dashboard dans Grafana.

**Cr√©ation d'un dashboard dans Grafana UI**:

Pour cr√©er un dashboard, nous pouvons simplement aller sur http://localhost:3000, nous connecter avec les identifiants par d√©faut "admin/admin" et cr√©er un nouveau tableau de bord depuis *Home -> Dashboards* dans *General* ou tout autre dossier.

Cliquez sur *Add visualization* pour ajouter de nouveaux diagrammes √† un dashboard.

Pour notre dashboard, nous avons ajout√© trois sch√©mas simples : *Requests per hour*, *Request proportions* et *Scrape duration*.

1. *Requests per hour*:

Sur la base de la m√©trique `http_requests_total` que nous avons cr√©√©e, cette visualisation montre un taux de chaque type de requ√™te dans une plage d'une heure. Le type de requ√™te d√©pend des deux labels que nous avons ajout√©es dans notre application python¬†: "method" et "code". Cela nous permet essentiellement de voir combien de fois par heure chaque demande est effectu√©e √† diff√©rents moments.

2. *Request proportions*:

Cette visualisation est un graphique en camembert bas√© sur la m√™me m√©trique `http_requests_total`, qui compare le nombre total de diff√©rents types de requ√™tes.

3. *Scrape duration*:

Cette visualisation est bas√©e sur la m√©trique interne Prometheus `scrape_duration_seconds` qui stocke une dur√©e de chaque "scrape", c√†d de chaque collecte des m√©triques depuis notre endpoint `/metrics`.